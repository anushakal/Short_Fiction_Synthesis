# Short_Fiction_Synthesis

## Overview
This project utilizes Natural Language Processing (NLP) strategies to generate short stories from fanfiction archives. Leveraging HuggingFace tools, specifically Recurrent Neural Networks (RNNs) and Transformers, we aim to extract captivating and intriguing tales from a diverse range of fan-created content.

### Team Members
- Anusha Kalbande
- Aakash Mahesha
- Rishabh Raj

## Table of Contents
- [Abstract](#abstract)
- [Project Introduction](#project-introduction)
- [Conclusion](#conclusion)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Evaluation](#evaluation)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## Abstract

This project harnesses NLP models, specifically Recurrent Neural Networks (RNNs) and Transformers from HuggingFace, to generate short stories from fanfiction archives. The HuggingFace API dataset serves as our primary data source. The effectiveness of the models is evaluated through manual assessment, word clouds for vocabulary analysis, and train-validate loss monitoring graphs.

## Project Introduction

Literature has been a vital aspect of human expression, offering diverse forms like poetry, prose, and short stories. Recent advancements in Natural Language Processing (NLP) enable the generation and analysis of literary texts. This project aims to revolutionize storytelling by using NLP to extract short tales from fanfiction archives. Fanfiction, a unique form of writing, provides a rich source of data for NLP projects, offering a variety of stories, characters, locations, and themes.

## Conclusion

### LSTMs
The stories generated by the LSTM model lacked coherence, suggesting a failure to capture contextual nuances.

### GPT-2
GPT-2 outperformed LSTM, generating more coherent and contextually appropriate stories. Its ability to capture long-term context and dependencies in text data, coupled with its large-scale pretraining, contributed to superior results.
